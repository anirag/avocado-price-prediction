{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "> In this notebook, I have built time series models aimed to forecast avocado prices in a region. Two models have benn used Prophet (from facebook) and ARIMA for this purpose. \n",
    "> This notebook also demonstrates two methods of validating a model\n",
    ">* **TSCV**: Time series cross validation, where the data is split into train and test using a time column. An example split is shown below\n",
    "\n",
    "    DATA: [0 1 2 3 4 5]\n",
    "\n",
    "    TRAIN: [0] TEST: [1]\n",
    "    TRAIN: [0 1] TEST: [2]\n",
    "    TRAIN: [0 1 2] TEST: [3]\n",
    "    TRAIN: [0 1 2 3] TEST: [4]\n",
    "    TRAIN: [0 1 2 3 4] TEST: [5]\n",
    "\n",
    ">* **One step forecast**: In this case Data is jsut split into train and test. Then a model is built on train data and one step forecast is made. Then the model is updated with that forecast as if it was the observed value and then prediction for next time period is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:49:04.482791Z",
     "start_time": "2019-11-07T17:49:04.475558Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import janitor\n",
    "\n",
    "# PROPHET\n",
    "from plotnine import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit #Splitting for time series CV!\n",
    "from fbprophet import Prophet \n",
    "\n",
    "# ARIMA\n",
    "import pmdarima as pm\n",
    "from pandas.plotting import lag_plot\n",
    "from pmdarima.arima import ndiffs\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:49:04.709388Z",
     "start_time": "2019-11-07T17:49:04.676450Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"avocado.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:49:04.914147Z",
     "start_time": "2019-11-07T17:49:04.889665Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing index column\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# Removing records with TotalUS region, assuming it is nust the average of all other regions\n",
    "df = df.loc[df.region!='TotalUS'].reset_index(drop=True)\n",
    "\n",
    "# Making date to datetime and sorting chrinologically\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values('Date')\n",
    "df = df.clean_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:49:05.153865Z",
     "start_time": "2019-11-07T17:49:05.139983Z"
    }
   },
   "outputs": [],
   "source": [
    "denver_conventional = df.loc[(df.region=='Denver')&(df.type=='conventional')].reset_index(drop=True)\n",
    "denver_organic = df.loc[(df.region=='Denver')&(df.type=='organic')].reset_index(drop=True)\n",
    "denver_conventional.shape,denver_organic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://facebook.github.io/prophet/\n",
    "\n",
    "After expermienting with seasonality, adding/removing regressors, the final model was chosen to be the model with weekly seasonality with few regressors. The rmse error dropped from 0.31(no seasonality) to 0.2(weekly with regressors) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:11:12.362217Z",
     "start_time": "2019-11-07T17:11:12.359647Z"
    }
   },
   "source": [
    "## Conventional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:49:06.784573Z",
     "start_time": "2019-11-07T17:49:06.779240Z"
    }
   },
   "outputs": [],
   "source": [
    "prophet_df=denver_conventional.rename(columns={\"date\": \"ds\", \"averageprice\": \"y\"})\n",
    "prophet_df=prophet_df.sort_values(\"ds\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:49:07.694433Z",
     "start_time": "2019-11-07T17:49:07.691579Z"
    }
   },
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:49:08.191379Z",
     "start_time": "2019-11-07T17:49:08.187744Z"
    }
   },
   "outputs": [],
   "source": [
    "prophet_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:49:21.169856Z",
     "start_time": "2019-11-07T17:49:08.771902Z"
    }
   },
   "outputs": [],
   "source": [
    "output=pd.DataFrame()\n",
    "for i,(train_index,test_index) in enumerate(tscv.split(prophet_df)):\n",
    "    print(f\"FOLD: {i}\")\n",
    "    final_df = pd.DataFrame()\n",
    "    train_df=prophet_df.copy().iloc[train_index,:]\n",
    "    test_df=prophet_df.copy().iloc[test_index,:]\n",
    "\n",
    "    m=Prophet(seasonality_mode='multiplicative',\n",
    "            yearly_seasonality=False,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False\n",
    "        )\n",
    "    \n",
    "    m.add_regressor('total_volume')\n",
    "    #m.add_regressor('4046')\n",
    "    #m.add_regressor('4225')\n",
    "    m.add_regressor('4770')\n",
    "    m.add_regressor('total_bags')\n",
    "    #m.add_regressor('small_bags')\n",
    "    m.add_regressor('large_bags')\n",
    "    m.add_regressor('xlarge_bags')\n",
    "    \n",
    "    m.fit(train_df)\n",
    "    predictions=m.predict(test_df)\n",
    "    pred_df=predictions.loc[:,[\"ds\",\"yhat\"]]\n",
    "    #pred_df['yhat'] = pred_df['yhat'].apply(lambda x: 0 if x<0 else x)\n",
    "    pred_df[\"y\"]=test_df.y.tolist()\n",
    "    pred_df[\"total_volume\"]=test_df.total_volume.tolist()\n",
    "    pred_df[\"4046\"]=test_df['4046'].tolist()\n",
    "    pred_df[\"4225\"]=test_df['4225'].tolist()\n",
    "    pred_df[\"4770\"]=test_df['4770'].tolist()\n",
    "    pred_df[\"total_bags\"]=test_df.total_bags.tolist()\n",
    "    pred_df[\"small_bags\"]=test_df.small_bags.tolist()\n",
    "    pred_df[\"large_bags\"]=test_df.large_bags.tolist()\n",
    "    pred_df[\"xlarge_bags\"]=test_df.xlarge_bags.tolist()\n",
    "    \n",
    "   \n",
    "    train_df[\"indicator\"]=\"Train\"\n",
    "    pred_df[\"indicator\"]=\"Test\"\n",
    "    final_df=train_df.append(pred_df).reset_index(drop=True)\n",
    "    final_df[\"fold\"]=\"Fold \"+str(i+1)\n",
    "    final_df[\"rmse\"]=np.sqrt((np.mean((final_df.yhat-final_df.y)**2)))\n",
    "    output = output.append(final_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:51:33.252388Z",
     "start_time": "2019-11-07T17:49:21.172302Z"
    }
   },
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import cross_validation\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "cross_validation_results = cross_validation(m, initial='210 days', period='15 days', horizon='70 days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:51:34.226353Z",
     "start_time": "2019-11-07T17:51:33.254658Z"
    }
   },
   "outputs": [],
   "source": [
    "(ggplot(output,aes(\"ds\",\"y\",color=\"factor(indicator)\"))+\\\n",
    " geom_point()+facet_grid('fold~.'))+\\\n",
    "labs(title=\"Train/Test Splits\",x=\"Date\",y=\"Price\")+\\\n",
    "scale_x_date(date_breaks=\"6 months\",date_labels =  \"%b %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:51:34.238474Z",
     "start_time": "2019-11-07T17:51:34.228506Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output.groupby('fold').agg({'rmse':'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:51:34.246414Z",
     "start_time": "2019-11-07T17:51:34.241215Z"
    }
   },
   "outputs": [],
   "source": [
    "prophet_df=denver_organic.rename(columns={\"date\": \"ds\", \"averageprice\": \"y\"})\n",
    "prophet_df=prophet_df.sort_values(\"ds\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:51:34.250641Z",
     "start_time": "2019-11-07T17:51:34.248324Z"
    }
   },
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:51:34.256781Z",
     "start_time": "2019-11-07T17:51:34.252630Z"
    }
   },
   "outputs": [],
   "source": [
    "prophet_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:51:44.386268Z",
     "start_time": "2019-11-07T17:51:34.258439Z"
    }
   },
   "outputs": [],
   "source": [
    "output=pd.DataFrame()\n",
    "for i,(train_index,test_index) in enumerate(tscv.split(prophet_df)):\n",
    "    print(f\"FOLD: {i}\")\n",
    "    final_df = pd.DataFrame()\n",
    "    train_df=prophet_df.copy().iloc[train_index,:]\n",
    "    test_df=prophet_df.copy().iloc[test_index,:]\n",
    "\n",
    "    m=Prophet(\n",
    "            yearly_seasonality=False,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False\n",
    "        )\n",
    "    \n",
    "    m.add_regressor('total_volume')\n",
    "    #m.add_regressor('4046')\n",
    "    #m.add_regressor('4225')\n",
    "    m.add_regressor('4770')\n",
    "    m.add_regressor('total_bags')\n",
    "    #m.add_regressor('small_bags')\n",
    "    m.add_regressor('large_bags')\n",
    "    m.add_regressor('xlarge_bags')\n",
    "    \n",
    "    m.fit(train_df)\n",
    "    predictions=m.predict(test_df)\n",
    "    pred_df=predictions.loc[:,[\"ds\",\"yhat\"]]\n",
    "    pred_df['yhat'] = pred_df['yhat'].apply(lambda x: 0 if x<0 else x)\n",
    "    pred_df[\"y\"]=test_df.y.tolist()\n",
    "    pred_df[\"total_volume\"]=test_df.total_volume.tolist()\n",
    "    pred_df[\"4046\"]=test_df['4046'].tolist()\n",
    "    pred_df[\"4225\"]=test_df['4225'].tolist()\n",
    "    pred_df[\"4770\"]=test_df['4770'].tolist()\n",
    "    pred_df[\"total_bags\"]=test_df.total_bags.tolist()\n",
    "    pred_df[\"small_bags\"]=test_df.small_bags.tolist()\n",
    "    pred_df[\"large_bags\"]=test_df.large_bags.tolist()\n",
    "    pred_df[\"xlarge_bags\"]=test_df.xlarge_bags.tolist()\n",
    "    \n",
    "   \n",
    "    train_df[\"indicator\"]=\"Train\"\n",
    "    pred_df[\"indicator\"]=\"Test\"\n",
    "    final_df=train_df.append(pred_df).reset_index(drop=True)\n",
    "    final_df[\"fold\"]=\"Fold \"+str(i+1)\n",
    "    final_df[\"rmse\"]=np.sqrt((np.mean((final_df.yhat-final_df.y)**2)))\n",
    "    output = output.append(final_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:51:45.304644Z",
     "start_time": "2019-11-07T17:51:44.388506Z"
    }
   },
   "outputs": [],
   "source": [
    "(ggplot(output,aes(\"ds\",\"y\",color=\"factor(indicator)\"))+\\\n",
    " geom_point()+facet_grid('fold~.'))+\\\n",
    "labs(title=\"Train/Test Splits\",x=\"Date\",y=\"Price\")+\\\n",
    "scale_x_date(date_breaks=\"6 months\",date_labels =  \"%b %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:51:45.316784Z",
     "start_time": "2019-11-07T17:51:45.306857Z"
    }
   },
   "outputs": [],
   "source": [
    "output.groupby('fold').agg({'rmse':'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let us use ARIMA model to forecast prices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:53:41.558408Z",
     "start_time": "2019-11-07T17:53:41.554969Z"
    }
   },
   "outputs": [],
   "source": [
    "denver_conventional = denver_conventional.sort_values('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:54:31.633327Z",
     "start_time": "2019-11-07T17:54:31.631083Z"
    }
   },
   "source": [
    "### Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:53:41.983151Z",
     "start_time": "2019-11-07T17:53:41.978194Z"
    }
   },
   "outputs": [],
   "source": [
    "train_len = int(denver_conventional.shape[0] * 0.8)\n",
    "train_data, test_data = denver_conventional[:train_len], denver_conventional[train_len:]\n",
    "\n",
    "y_train = train_data['averageprice'].values\n",
    "y_test = test_data['averageprice'].values\n",
    "\n",
    "print(f\"{train_len} train samples\")\n",
    "print(f\"{denver_conventional.shape[0] - train_len} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for stationarity\n",
    "> Using autocorrelation plot see what lags are important. We see that before diffrencing we have first few lags correlated and p-value for stationarity test is 0.30 (high). But after differencing (pvalue~0) and still see some small spikes in lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:53:43.282380Z",
     "start_time": "2019-11-07T17:53:42.977869Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(211)\n",
    "\n",
    "# Plot the autocorrelation function\n",
    "plot_acf(y_train.squeeze(), lags=16, ax=ax)\n",
    "ax = plt.subplot(212)\n",
    "plot_pacf(y_train.squeeze(), lags=16, ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:58:19.171371Z",
     "start_time": "2019-11-07T17:58:18.351172Z"
    }
   },
   "outputs": [],
   "source": [
    "seasonal_decompose(y_train, model='additive',freq=4).plot()\n",
    "print(\"Dickey–Fuller test: p=%f\" % adfuller(y_train)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:53:44.284310Z",
     "start_time": "2019-11-07T17:53:44.281004Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_diff = train_data['averageprice'].diff()[1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:53:45.341455Z",
     "start_time": "2019-11-07T17:53:45.017837Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(211)\n",
    "\n",
    "# Plot the autocorrelation function\n",
    "plot_acf(y_train_diff.squeeze(), lags=16, ax=ax)\n",
    "ax = plt.subplot(212)\n",
    "plot_pacf(y_train_diff.squeeze(), lags=16, ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T17:59:39.546745Z",
     "start_time": "2019-11-07T17:59:38.741663Z"
    }
   },
   "outputs": [],
   "source": [
    "seasonal_decompose(y_train_diff, model='additive',freq=4).plot()\n",
    "print(\"Dickey–Fuller test: p=%f\" % adfuller(y_train_diff)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag Plot\n",
    "> Lag plot to see if the correlated lags are linear or non-linear. The plots show that the relationship between the lags are quite linear, so auto-regressive models will help in case of forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:02:46.656297Z",
     "start_time": "2019-11-07T18:02:45.526122Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 16))\n",
    "plt.title('Autocorrelation plot')\n",
    "\n",
    "# The axis coordinates for the plots\n",
    "ax_idcs = [\n",
    "    (0, 0),\n",
    "    (0, 1),\n",
    "    (1, 0),\n",
    "    (1, 1),\n",
    "    (2, 0),\n",
    "    (2, 1)\n",
    "]\n",
    "\n",
    "for lag, ax_coords in enumerate(ax_idcs, 1):\n",
    "    ax_row, ax_col = ax_coords\n",
    "    axis = axes[ax_row][ax_col]\n",
    "    lag_plot(denver_conventional['averageprice'], lag=lag, ax=axis)\n",
    "    axis.set_title(f\"Lag={lag}\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All lags look fairly linear, so it's a good indicator that an auto-regressive model is a good choice. Therefore, we'll allow the auto_arima to select the lag term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:02:50.975617Z",
     "start_time": "2019-11-07T18:02:50.964485Z"
    }
   },
   "outputs": [],
   "source": [
    "from pmdarima.arima import ndiffs\n",
    "\n",
    "kpss_diffs = ndiffs(y_train, alpha=0.05, test='kpss', max_d=6)\n",
    "adf_diffs = ndiffs(y_train, alpha=0.05, test='adf', max_d=6)\n",
    "n_diffs = max(adf_diffs, kpss_diffs)\n",
    "\n",
    "print(f\"Estimated differencing term: {n_diffs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto - ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:03:41.714096Z",
     "start_time": "2019-11-07T18:03:04.676282Z"
    }
   },
   "outputs": [],
   "source": [
    "auto = pm.auto_arima(y_train, start_p=1, start_q=1,\n",
    "                           max_p=6, max_q=6, m=12,\n",
    "                           start_P=0, seasonal=True,\n",
    "                           d=1, D=1, trace=True,\n",
    "                           error_action='ignore',  \n",
    "                           suppress_warnings=True, \n",
    "                           stepwise=True,random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:03:41.735284Z",
     "start_time": "2019-11-07T18:03:41.717245Z"
    }
   },
   "outputs": [],
   "source": [
    "auto.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:03:41.741502Z",
     "start_time": "2019-11-07T18:03:41.737944Z"
    }
   },
   "outputs": [],
   "source": [
    "print(auto.order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **As we saw in the autocorrelation plots, lag 2 had highest correlation and auto_arima had chosen order of 2 for AR model as expected**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting\n",
    "> As described earlier, I will use the model from auto_arima to make one-step forecast and then update the model with the recent forecast and then move on to next time period. The rmse error is ~0.1 in this case lower than prophet. WE can also update the model with actual value before making the next forecast, which will improve hte model even better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:05:45.710109Z",
     "start_time": "2019-11-07T18:05:34.703003Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = auto\n",
    "\n",
    "def forecast_one_step():\n",
    "    fc, conf_int = model.predict(n_periods=1, return_conf_int=True)\n",
    "    return (\n",
    "        fc.tolist()[0],\n",
    "        np.asarray(conf_int).tolist()[0])\n",
    "\n",
    "forecasts = []\n",
    "confidence_intervals = []\n",
    "\n",
    "for new_ob in y_test:\n",
    "    fc, conf = forecast_one_step()\n",
    "    forecasts.append(fc)\n",
    "    confidence_intervals.append(conf)\n",
    "    \n",
    "    # Updates the existing model with a small number of MLE steps\n",
    "    model.update([fc])\n",
    "    \n",
    "print(f\"Mean squared error: {mean_squared_error(y_test, forecasts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:05:46.299506Z",
     "start_time": "2019-11-07T18:05:45.712908Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 12))\n",
    "axes[0].plot(y_train, color='blue', label='Training Data')\n",
    "axes[0].plot(test_data.index, forecasts, color='green', marker='o',label='Predicted Price')\n",
    "\n",
    "axes[0].plot(test_data.index, y_test, color='red', label='Actual Price')\n",
    "axes[0].set_title('Denver Avocado Prices Prediction - Conventional')\n",
    "axes[0].set_xlabel('Dates')\n",
    "axes[0].set_ylabel('Prices')\n",
    "axes[0].legend()\n",
    "\n",
    "\n",
    "axes[1].plot(y_train, color='blue', label='Training Data')\n",
    "axes[1].plot(test_data.index, forecasts, color='green',\n",
    "             label='Predicted Price')\n",
    "\n",
    "axes[1].set_title('Prices Predictions & Confidence Intervals')\n",
    "axes[1].set_xlabel('Dates')\n",
    "axes[1].set_ylabel('Prices')\n",
    "\n",
    "conf_int = np.asarray(confidence_intervals)\n",
    "axes[1].fill_between(test_data.index,\n",
    "                     conf_int[:, 0], conf_int[:, 1],\n",
    "                     alpha=0.9, color='orange',\n",
    "                     label=\"Confidence Intervals\")\n",
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:05:46.306143Z",
     "start_time": "2019-11-07T18:05:46.301830Z"
    }
   },
   "outputs": [],
   "source": [
    "denver_organic = denver_organic.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:05:46.313317Z",
     "start_time": "2019-11-07T18:05:46.308526Z"
    }
   },
   "outputs": [],
   "source": [
    "train_len = int(denver_organic.shape[0] * 0.8)\n",
    "train_data, test_data = denver_organic[:train_len], denver_organic[train_len:]\n",
    "\n",
    "y_train = train_data['averageprice'].values\n",
    "y_test = test_data['averageprice'].values\n",
    "\n",
    "print(f\"{train_len} train samples\")\n",
    "print(f\"{denver_conventional.shape[0] - train_len} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:05:47.289018Z",
     "start_time": "2019-11-07T18:05:46.316296Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 16))\n",
    "plt.title('Autocorrelation plot')\n",
    "\n",
    "# The axis coordinates for the plots\n",
    "ax_idcs = [\n",
    "    (0, 0),\n",
    "    (0, 1),\n",
    "    (1, 0),\n",
    "    (1, 1),\n",
    "    (2, 0),\n",
    "    (2, 1)\n",
    "]\n",
    "\n",
    "for lag, ax_coords in enumerate(ax_idcs, 1):\n",
    "    ax_row, ax_col = ax_coords\n",
    "    axis = axes[ax_row][ax_col]\n",
    "    lag_plot(denver_organic['averageprice'], lag=lag, ax=axis)\n",
    "    axis.set_title(f\"Lag={lag}\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:05:47.302072Z",
     "start_time": "2019-11-07T18:05:47.291077Z"
    }
   },
   "outputs": [],
   "source": [
    "from pmdarima.arima import ndiffs\n",
    "\n",
    "kpss_diffs = ndiffs(y_train, alpha=0.05, test='kpss', max_d=6)\n",
    "adf_diffs = ndiffs(y_train, alpha=0.05, test='adf', max_d=6)\n",
    "n_diffs = max(adf_diffs, kpss_diffs)\n",
    "\n",
    "print(f\"Estimated differencing term: {n_diffs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:07:16.122880Z",
     "start_time": "2019-11-07T18:05:47.304383Z"
    }
   },
   "outputs": [],
   "source": [
    "auto = pm.auto_arima(y_train, start_p=1, start_q=1,\n",
    "                           max_p=6, max_q=6, m=12,\n",
    "                           start_P=0, seasonal=True,\n",
    "                           d=1, D=1, trace=True,\n",
    "                           error_action='ignore',  \n",
    "                           suppress_warnings=True, \n",
    "                           stepwise=True,random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:07:16.144356Z",
     "start_time": "2019-11-07T18:07:16.125384Z"
    }
   },
   "outputs": [],
   "source": [
    "auto.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:07:16.150560Z",
     "start_time": "2019-11-07T18:07:16.146954Z"
    }
   },
   "outputs": [],
   "source": [
    "print(auto.order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:07:46.906468Z",
     "start_time": "2019-11-07T18:07:16.153575Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = auto\n",
    "\n",
    "def forecast_one_step():\n",
    "    fc, conf_int = model.predict(n_periods=1, return_conf_int=True)\n",
    "    return (\n",
    "        fc.tolist()[0],\n",
    "        np.asarray(conf_int).tolist()[0])\n",
    "\n",
    "forecasts = []\n",
    "confidence_intervals = []\n",
    "\n",
    "for new_ob in y_test:\n",
    "    fc, conf = forecast_one_step()\n",
    "    forecasts.append(fc)\n",
    "    confidence_intervals.append(conf)\n",
    "    \n",
    "    # Updates the existing model with a small number of MLE steps\n",
    "    model.update([fc])\n",
    "    \n",
    "print(f\"Mean squared error: {mean_squared_error(y_test, forecasts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:07:47.545774Z",
     "start_time": "2019-11-07T18:07:46.908929Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 12))\n",
    "axes[0].plot(y_train, color='blue', label='Training Data')\n",
    "axes[0].plot(test_data.index, forecasts, color='green', marker='o',label='Predicted Price')\n",
    "\n",
    "axes[0].plot(test_data.index, y_test, color='red', label='Actual Price')\n",
    "axes[0].set_title('Denver Avocado Prices Prediction - Organic')\n",
    "axes[0].set_xlabel('Dates')\n",
    "axes[0].set_ylabel('Prices')\n",
    "axes[0].legend()\n",
    "\n",
    "\n",
    "axes[1].plot(y_train, color='blue', label='Training Data')\n",
    "axes[1].plot(test_data.index, forecasts, color='green',\n",
    "             label='Predicted Price')\n",
    "\n",
    "axes[1].set_title('Prices Predictions & Confidence Intervals')\n",
    "axes[1].set_xlabel('Dates')\n",
    "axes[1].set_ylabel('Prices')\n",
    "\n",
    "conf_int = np.asarray(confidence_intervals)\n",
    "axes[1].fill_between(test_data.index,\n",
    "                     conf_int[:, 0], conf_int[:, 1],\n",
    "                     alpha=0.9, color='orange',\n",
    "                     label=\"Confidence Intervals\")\n",
    "axes[1].legend()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
