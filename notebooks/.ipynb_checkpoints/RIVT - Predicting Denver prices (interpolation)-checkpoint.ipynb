{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train a model to predict the avocado price in Denver using avocado prices from all other regions. Explain how you evaluate the performance of the model and relative importance of its features.\n",
    "\n",
    "> I am using all the given features and all records of given region to predict prices in a different region. Features used are all the columns given in the dataset plus lagged values of price and volume. This approach assumes that you have all feature information except for the price in the desired region. Linear Regression, Random Forest and Xgboost were used for training and XGBOOST was selected based on the RMSE and MAPE metrics. The final model is presented in a separate python file.\n",
    "\n",
    "> This might not bea best use-case for modeling. Single time series forecasting is done in a different notebook. Also, this notebook shows how to evaluate models using different metrics and get feature importance given a model and also error analysis using SHAPLEY values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T16:06:15.540277Z",
     "start_time": "2019-11-07T16:06:15.533990Z"
    }
   },
   "outputs": [],
   "source": [
    "# import packages for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import janitor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T16:06:15.780727Z",
     "start_time": "2019-11-07T16:06:15.728253Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"avocado.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T16:06:16.022204Z",
     "start_time": "2019-11-07T16:06:15.997396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing index column\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# Removing records with TotalUS region, assuming it is nust the average of all other regions\n",
    "df = df.loc[df.region!='TotalUS'].reset_index(drop=True)\n",
    "\n",
    "# Making date to datetime and sorting chrinologically\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['region','Date'])\n",
    "df = df.clean_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T16:06:17.472131Z",
     "start_time": "2019-11-07T16:06:17.177000Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adding month and day variable to visualize seasonal patterns\n",
    "df['month']=df['date'].apply(lambda x:x.month)\n",
    "df['day']=df['date'].apply(lambda x:x.day)\n",
    "#df['Week_Number'] = df['date'].dt.week\n",
    "#df['lag_y'] = df['averageprice'].shift()\n",
    "#df = df.dropna()\n",
    "for lag in range(1,11):\n",
    "    df[f'lag_{lag}'] = df.groupby(['region'])['averageprice'].shift(lag)\n",
    "for lag in range(1,11):\n",
    "    df[f'volume_lag_{lag}'] = df.groupby(['region'])['total_volume'].shift(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T16:07:52.311880Z",
     "start_time": "2019-11-07T16:07:51.134180Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,15));\n",
    "sns.heatmap(df.corr(),cmap='coolwarm',annot=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Most numerical columns are negatively correlated with price as they are variables associated with demand. Also lag2 is correlated the most with target variable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T16:08:08.143067Z",
     "start_time": "2019-11-07T16:08:08.132135Z"
    }
   },
   "outputs": [],
   "source": [
    "test = df.loc[df.region=='Denver'].reset_index(drop=True)\n",
    "train = df.loc[df.region!='Denver'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T16:08:08.821053Z",
     "start_time": "2019-11-07T16:08:08.814384Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def preprocessing(df, train=True):\n",
    "    \n",
    "    df = pd.get_dummies(df, columns = ['type'], prefix=['type'])\n",
    "    num_columns = ['total_volume','4046','4225','4770','total_bags','small_bags', 'large_bags', 'xlarge_bags']\n",
    "\n",
    "    if train:\n",
    "        sc = StandardScaler()\n",
    "        scaled_columns = sc.fit_transform(df[num_columns])\n",
    "        scaled_df = pd.DataFrame(scaled_columns)\n",
    "        scaled_df.columns = num_columns\n",
    "        df = df.drop(num_columns,axis=1).join(scaled_df)\n",
    "        pickle.dump(sc,open('scaler.p','wb'))\n",
    "        \n",
    "    else:\n",
    "        sc = pickle.load(open('scaler.p','rb'))\n",
    "        scaled_columns = sc.transform(df[num_columns])\n",
    "        scaled_df = pd.DataFrame(scaled_columns)\n",
    "        scaled_df.columns = num_columns\n",
    "        df = df.drop(num_columns,axis=1).join(scaled_df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T16:08:09.657902Z",
     "start_time": "2019-11-07T16:08:09.626297Z"
    }
   },
   "outputs": [],
   "source": [
    "train = preprocessing(train,train=True)\n",
    "test = preprocessing(test,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T16:08:10.813290Z",
     "start_time": "2019-11-07T16:08:10.806378Z"
    }
   },
   "outputs": [],
   "source": [
    "y = train['averageprice']\n",
    "date = train['date']\n",
    "regions = train['region']\n",
    "X = train.drop(['date','averageprice','region'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T16:08:11.720722Z",
     "start_time": "2019-11-07T16:08:11.716237Z"
    }
   },
   "outputs": [],
   "source": [
    "X.shape,y.shape,regions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T16:08:12.681149Z",
     "start_time": "2019-11-07T16:08:12.668718Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_modeling(X, y, regions, test, n_folds, model):\n",
    "    predictions=[]\n",
    "    denver_preds = np.zeros(len(test))\n",
    "    if model == 'linear':\n",
    "        model = LinearRegression()\n",
    "        title = 'Linear Regression'\n",
    "    if model == 'RF':\n",
    "        model = RandomForestRegressor(n_estimators=100)\n",
    "        title = 'Random Forest'\n",
    "    if model == 'Booster':\n",
    "        \n",
    "        params = {'colsample_bytree': 0.8, \n",
    "                  'gamma': 0.3, \n",
    "                  'max_depth': 7, \n",
    "                  'min_child_weight': 4, \n",
    "                  'subsample': 0.6,\n",
    "                 'learning_rate':0.02,\n",
    "                 'n_estimators':10000}\n",
    "        model = XGBRegressor(**params)\n",
    "        title = 'XGBoost'\n",
    "    \n",
    "    folds = GroupKFold(n_splits=n_folds)\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(folds.split(X, y, regions)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        date_train, date_test = date[train_index], date[test_index]\n",
    "        region_train, region_test = regions[train_index], regions[test_index]\n",
    "        eval_set =  [(X_test, y_test)]\n",
    "        if model=='Booster':\n",
    "            model.fit(X_train,y_train,eval_metric=\"rmse\", eval_set=eval_set, early_stopping_rounds=100,verbose=200)\n",
    "        else:\n",
    "            model.fit(X_train,y_train)\n",
    "        pred=model.predict(X_test)\n",
    "        print(f\"Validation RMSE for fold {i}: {round(np.sqrt(mean_squared_error(y_test,pred)),3)}\")\n",
    "        predictions.append(pd.DataFrame({'date': date_test,'region':region_test,'actual_price':y_test, 'predicted_price':pred}))\n",
    "        denver_preds += model.predict(test[X.columns])/5\n",
    "        \n",
    "    test['predicted_price'] = denver_preds\n",
    "    test['error'] = test.averageprice - test.predicted_price\n",
    "    mape = np.round(np.mean(np.abs(100*test.error/test.averageprice)), 2) \n",
    "    rmse = np.round(np.sqrt(mean_squared_error(test.averageprice,test.predicted_price)),2)\n",
    "    r2 = np.round(r2_score(test.averageprice,test.predicted_price),2)\n",
    "    plt.figure(figsize=(10,5));\n",
    "    sns.lineplot(x='date', y='value', hue='variable', data=pd.melt(test[['date','averageprice','predicted_price']], ['date']), err_style=None);\n",
    "    plt.title(f'{title}\\nActual vs Predicted Avocado price for Denver \\n RMSE: {rmse} \\n MAPE: {mape}% \\n R2: {r2}');\n",
    "    \n",
    "    return model,predictions,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:18:55.342060Z",
     "start_time": "2019-11-07T18:18:55.339752Z"
    }
   },
   "outputs": [],
   "source": [
    "#lm_model,lm_predictions,lm_test = custom_modeling(X, y, regions, test, 3, 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:18:58.317383Z",
     "start_time": "2019-11-07T18:18:58.315012Z"
    }
   },
   "outputs": [],
   "source": [
    "#rf_model,rf_predictions,rf_test = custom_modeling(X, y, regions, test, 5, 'RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T16:44:01.784431Z",
     "start_time": "2019-11-07T16:08:25.753805Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb_model,xgb_predictions,xgb_test = custom_modeling(X, y, regions, test, 5, 'Booster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is a forecasting problem where we are trying to predict avocado prices. We can evaluate the model using the following metrics.\n",
    ">* **R2 score** : The values for R2 range from 0 to 1, which captures the percentage of squared correlation between the predicted and actual values of the target variable.\n",
    ">* **MAPE**: Mean absolute percentage error, For each record, the absolute error is divided by the actual value, giving relative error\n",
    ">* **RMSE**: Root mean squared error, For each record, this measures how far the predicted value is from true error. \n",
    "\n",
    "> Based on all the above errors, XGBoost (ensemble decision tress) outperforms other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let us use **Gain** score to measure the relative importance of the features in the model. **‘Gain’** is the improvement in accuracy metric brought by a feature to the branches it is on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:12:05.737142Z",
     "start_time": "2019-11-07T18:12:05.633937Z"
    }
   },
   "outputs": [],
   "source": [
    "importance = xgb_model.get_booster().get_score(importance_type= 'gain')\n",
    "importance_df = pd.DataFrame(list(importance.items()), columns = ['feature','importance'])\n",
    "importance_df = importance_df.sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:12:06.406289Z",
     "start_time": "2019-11-07T18:12:05.913979Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,10));\n",
    "sns.barplot(importance_df.importance,importance_df.feature)\n",
    "#plt.bar(importance_df.feature, importance_df.importance)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T20:14:10.282781Z",
     "start_time": "2019-11-06T20:14:10.280143Z"
    }
   },
   "source": [
    "# SHAPley values explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:12:42.220105Z",
     "start_time": "2019-11-07T18:12:17.660305Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import warnings\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "expected_value = explainer.expected_value\n",
    "if isinstance(expected_value, list):\n",
    "    expected_value = expected_value[1]\n",
    "print(f\"Explainer expected value: {expected_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:14:44.773887Z",
     "start_time": "2019-11-07T18:12:42.223362Z"
    }
   },
   "outputs": [],
   "source": [
    "X_features = test[X.columns]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    shap_values = explainer.shap_values(X_features)\n",
    "    shap_interaction_values = explainer.shap_interaction_values(X_features)\n",
    "if isinstance(shap_interaction_values, list):\n",
    "    shap_interaction_values = shap_interaction_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:14:45.910222Z",
     "start_time": "2019-11-07T18:14:44.776096Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.decision_plot(expected_value, shap_values, X_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let us use Shapley values, mainly decision_plot to understand the working of our model and feature importances. \n",
    "The Decision plots shows th following \n",
    "* The x-axis represents the model's output. The y-axis lists the model's features. By default, the features are ordered by descending importance.The importance is calculated over the observations plotted. _This is usually different than the importance ordering for the entire dataset.\n",
    "* The plot is centered on the x-axis at explainer.expected_value. All SHAP values are relative to the model's expected value like a linear model's effects are relative to the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** Looking at the plot, shapley values importances more or less agrees with the 'gain' score from xgboost importance. The most helpful insight from this plot is the 'typical' prediction path. The features at the bottom have zero to no effect in predicting the price month and year have some effect but still closer to the expected values. Real effect can be seen by the lag2, large_bags and volume. The predictions sway directions based in these features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:17:40.847380Z",
     "start_time": "2019-11-07T18:17:37.164805Z"
    }
   },
   "outputs": [],
   "source": [
    "select = range(10)\n",
    "X_features = test[X.columns].iloc[select]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    shap_values = explainer.shap_values(X_features)\n",
    "    shap_interaction_values = explainer.shap_interaction_values(X_features)\n",
    "if isinstance(shap_interaction_values, list):\n",
    "    shap_interaction_values = shap_interaction_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:17:41.347769Z",
     "start_time": "2019-11-07T18:17:40.849932Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.decision_plot(expected_value, shap_values, X_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** This is a really great way to isolate records where we see that the error is high and checking which feature had the most effect**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Records where difference between prediction and actual is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:18:29.875112Z",
     "start_time": "2019-11-07T18:18:27.603527Z"
    }
   },
   "outputs": [],
   "source": [
    "error_indices = xgb_test.loc[(xgb_test.error<-0.4)|(xgb_test.error>0.2)].index.values\n",
    "X_features = test[X.columns].iloc[error_indices]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    shap_values = explainer.shap_values(X_features)\n",
    "    shap_interaction_values = explainer.shap_interaction_values(X_features)\n",
    "if isinstance(shap_interaction_values, list):\n",
    "    shap_interaction_values = shap_interaction_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:18:29.891838Z",
     "start_time": "2019-11-07T18:18:29.877621Z"
    }
   },
   "outputs": [],
   "source": [
    "test.iloc[error_indices][['date','year','month','region','type_conventional','averageprice','predicted_price','error']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:18:35.673303Z",
     "start_time": "2019-11-07T18:18:35.195390Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.decision_plot(expected_value, shap_values, X_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at the plot and examples of the cases where the model made a huge error, we can see that most of the records have price under $1 for 'organic' avocados which is an outlier. This pattern of low price for organic avocados is not captured by the model. One way to rectify is to build separate model so that model can rely on other features such as month, volume to understand the cases of lower prices in organic type.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T21:30:07.940275Z",
     "start_time": "2019-11-06T21:12:39.674423Z"
    }
   },
   "outputs": [],
   "source": [
    "# Various hyper-parameters to tune\n",
    "xgb1 = XGBRegressor()\n",
    "folds = GroupKFold(n_splits=5)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'min_child_weight':[4,5], \n",
    "          'gamma':[i/10.0 for i in range(3,6)], \n",
    "          'subsample':[i/10.0 for i in range(6,11)],\n",
    "          'colsample_bytree':[i/10.0 for i in range(6,11)], \n",
    "          'max_depth': [4,5,6,7]}\n",
    "         #'reg_alpha':[0.001,0.01,0.1,1,10,100], \n",
    "         #'reg_lambda':[0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb1,\n",
    "                        params,\n",
    "                        cv = folds.split(X, y, regions),\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n",
    "#xgb_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T21:31:17.306464Z",
     "start_time": "2019-11-06T21:31:17.302504Z"
    }
   },
   "outputs": [],
   "source": [
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
